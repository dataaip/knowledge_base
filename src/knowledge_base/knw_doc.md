æˆ‘æƒ³è¦å°† cpprefence ç½‘ç«™ çˆ¬è™«ä¸‹æ¥ è¿›è¡Œç¿»è¯‘ä¸ºä¸­æ–‡ï¼Œå†è¿›è¡Œå¯¹æ¯ä¸€ç« èŠ‚å†…å®¹è¯¦è§£ä¸¾ä¾‹å¹¶æ€»ç»“ï¼Œç”Ÿæˆmdæ–‡æ¡£ï¼Œå½¢æˆè‡ªå·±çš„çŸ¥è¯†åº“ã€‚å¸®æˆ‘ç”Ÿæˆä¸€ä¸ªå®Œæ•´å¯è¡Œçš„å…·ä½“æ–¹æ¡ˆã€‚

---

### å®Œæ•´æ–¹æ¡ˆï¼šCppreference çˆ¬è™«ç¿»è¯‘ä¸çŸ¥è¯†åº“æ„å»º

#### æ•´ä½“æ¶æ„
```mermaid
graph TD
    A[çˆ¬è™«ç³»ç»Ÿ] --> B[å†…å®¹è§£æ]
    B --> C[ç¿»è¯‘å¼•æ“]
    C --> D[å†…å®¹å¢å¼º]
    D --> E[Markdownç”Ÿæˆ]
    E --> F[çŸ¥è¯†åº“å­˜å‚¨]
```

---

### å…·ä½“å®ç°æ­¥éª¤

#### 1. çˆ¬è™«ç³»ç»Ÿ (Python)
```python
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import time
import os

BASE_URL = "https://en.cppreference.com/w/"
OUTPUT_DIR = "raw_content"

def crawl_cppreference():
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # æ ¸å¿ƒçˆ¬å–å‡½æ•°
    def crawl_page(url, depth=0, max_depth=3):
        if depth > max_depth:
            return
            
        try:
            response = requests.get(url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ä¿å­˜åŸå§‹HTML
            filename = url.split("/")[-1] or "index.html"
            with open(f"{OUTPUT_DIR}/{filename}.html", "w", encoding="utf-8") as f:
                f.write(response.text)
            
            # æå–å†…å®¹åŒºåŸŸ (æ ¹æ®å®é™…é¡µé¢ç»“æ„è°ƒæ•´é€‰æ‹©å™¨)
            content = soup.select_one("#content")
            if not content:
                return None
                
            # é€’å½’çˆ¬å–å­é¡µé¢
            for link in content.select("a[href]"):
                href = link["href"]
                if href.startswith("/w/") and "#" not in href:
                    new_url = urljoin(BASE_URL, href)
                    time.sleep(1)  # ç¤¼è²Œçˆ¬å–
                    crawl_page(new_url, depth+1)
                    
            return str(content)
            
        except Exception as e:
            print(f"Error crawling {url}: {e}")
            return None

    # ä»ä¸»é¡µå¼€å§‹çˆ¬å–
    crawl_page(BASE_URL)

if __name__ == "__main__":
    crawl_cppreference()
```

#### 2. å†…å®¹è§£æå™¨ (Python)
```python
import re
from bs4 import BeautifulSoup

def parse_html_content(html):
    soup = BeautifulSoup(html, 'html.parser')
    
    # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
    for element in soup.select(".t-navbar, .t-dsc-see, .t-dsc-header"):
        element.decompose()
    
    # æå–æ ¸å¿ƒå†…å®¹
    title = soup.find("h1").text.strip()
    sections = []
    
    # è§£ææ¯ä¸ªç« èŠ‚
    for section in soup.select(".t-section, .t-dsc"):
        section_title = section.find("h2") or section.find("h3")
        if not section_title:
            continue
            
        # æå–ä»£ç ç¤ºä¾‹
        code_blocks = []
        for pre in section.select("pre"):
            code_blocks.append({
                "lang": "cpp",
                "code": pre.text.strip()
            })
            pre.decompose()
        
        # æå–æ–‡æœ¬å†…å®¹
        text_content = section.get_text(separator="\n", strip=True)
        text_content = re.sub(r'\n{3,}', '\n\n', text_content)
        
        sections.append({
            "title": section_title.text.strip(),
            "content": text_content,
            "code_blocks": code_blocks
        })
    
    return {
        "page_title": title,
        "sections": sections
    }
```

#### 3. ç¿»è¯‘å¼•æ“ (Python + DeepL API)
```python
import requests
import os

DEEPL_API_KEY = os.getenv("DEEPL_API_KEY")

def translate_text(text, target_lang="ZH"):
    """ä½¿ç”¨DeepL APIç¿»è¯‘æ–‡æœ¬"""
    endpoint = "https://api-free.deepl.com/v2/translate"
    
    response = requests.post(
        endpoint,
        data={
            "auth_key": DEEPL_API_KEY,
            "text": text,
            "target_lang": target_lang,
            "preserve_formatting": "1"
        },
        timeout=30
    )
    
    if response.status_code == 200:
        return response.json()["translations"][0]["text"]
    else:
        raise Exception(f"Translation failed: {response.text}")
```

#### 4. å†…å®¹å¢å¼ºæ¨¡å— (Python + GPT API)
```python
import openai

openai.api_key = os.getenv("OPENAI_API_KEY")

def enhance_content(title, content):
    """ä½¿ç”¨GPTè¿›è¡Œå†…å®¹è§£é‡Šå’Œæ‰©å±•"""
    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªC++ä¸“å®¶ï¼Œè¯·å¯¹ä»¥ä¸‹C++å‚è€ƒå†…å®¹è¿›è¡Œæ‰©å±•ï¼š
    1. æ·»åŠ é€šä¿—æ˜“æ‡‚çš„ä¸­æ–‡è§£é‡Š
    2. æä¾›1-2ä¸ªå®ç”¨ä»£ç ç¤ºä¾‹
    3. æ·»åŠ æ³¨æ„äº‹é¡¹å’Œä½¿ç”¨åœºæ™¯è¯´æ˜
    4. æœ€åç”¨2å¥è¯æ€»ç»“æ ¸å¿ƒè¦ç‚¹
    
    åŸå§‹å†…å®¹æ ‡é¢˜ï¼š{title}
    åŸå§‹å†…å®¹ï¼š
    {content}
    """
    
    response = openai.ChatCompletion.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.3,
        max_tokens=1500
    )
    
    return response.choices[0].message.content.strip()
```

#### 5. Markdownç”Ÿæˆå™¨ (Python)
```python
def generate_markdown(parsed_data):
    """ç”Ÿæˆç»“æ„åŒ–Markdownæ–‡æ¡£"""
    output = f"# {parsed_data['page_title']}\n\n"
    
    for section in parsed_data["sections"]:
        output += f"## {section['title']}\n\n"
        output += f"{section['translated_content']}\n\n"
        
        # æ·»åŠ å¢å¼ºå†…å®¹
        output += f"### âœ¨ æ·±å…¥è§£æ\n"
        output += f"{section['enhanced_content']}\n\n"
        
        # æ·»åŠ ä»£ç ç¤ºä¾‹
        if section["code_blocks"]:
            output += "### ä»£ç ç¤ºä¾‹\n"
            for i, code in enumerate(section["code_blocks"], 1):
                output += f"```cpp\n// ç¤ºä¾‹ {i}\n{code['code']}\n```\n\n"
        
        # æ·»åŠ æ€»ç»“
        output += "### ğŸ’¡ æ ¸å¿ƒè¦ç‚¹\n"
        output += f"{section['summary']}\n\n---\n\n"
    
    return output
```

#### 6. ä¸»å·¥ä½œæµ
```python
def main_workflow():
    # 1. çˆ¬å–å†…å®¹
    raw_html = crawl_cppreference()
    
    # 2. è§£æå†…å®¹
    parsed_data = parse_html_content(raw_html)
    
    # 3. ç¿»è¯‘å’Œå¢å¼º
    for section in parsed_data["sections"]:
        # ç¿»è¯‘ä¸»è¦å†…å®¹
        section["translated_content"] = translate_text(section["content"])
        
        # å¢å¼ºå†…å®¹
        enhanced = enhance_content(
            f"{parsed_data['page_title']} - {section['title']}",
            section["translated_content"]
        )
        
        # åˆ†å‰²å¢å¼ºå†…å®¹
        parts = enhanced.split("æ ¸å¿ƒè¦ç‚¹ï¼š")
        section["enhanced_content"] = parts[0]
        section["summary"] = parts[1] if len(parts) > 1 else ""
    
    # 4. ç”ŸæˆMarkdown
    md_content = generate_markdown(parsed_data)
    
    # 5. ä¿å­˜åˆ°çŸ¥è¯†åº“
    filename = f"knowledge_base/{parsed_data['page_title'].replace(' ', '_')}.md"
    with open(filename, "w", encoding="utf-8") as f:
        f.write(md_content)
```

---

### å…³é”®æ³¨æ„äº‹é¡¹

1. **çˆ¬è™«ä¼¦ç†**
   - éµå®ˆ `robots.txt` è§„åˆ™
   - è®¾ç½®è¯·æ±‚é—´éš”ï¼ˆâ‰¥1ç§’ï¼‰
   - ä½¿ç”¨ç¼“å­˜é¿å…é‡å¤çˆ¬å–
   - æ·»åŠ  `User-Agent` æ ‡è¯†

2. **ç¿»è¯‘è´¨é‡**
   - æŠ€æœ¯æœ¯è¯­ç»Ÿä¸€æ€§ï¼šåˆ›å»ºæœ¯è¯­å¯¹ç…§è¡¨
   - ä¿ç•™ä»£ç å’Œå…¬å¼åŸæ ¼å¼
   - äººå·¥å®¡æ ¸å…³é”®å†…å®¹

3. **å¢å¼ºå†…å®¹éªŒè¯**
   - GPTç”Ÿæˆå†…å®¹éœ€ç»è¿‡ç¼–è¯‘å™¨éªŒè¯
   - æ·»åŠ å…è´£å£°æ˜ï¼š"AIç”Ÿæˆå†…å®¹ä»…ä¾›å‚è€ƒ"
   - å®šæœŸæ›´æ–°çŸ¥è¯†åº“

4. **çŸ¥è¯†åº“ç»“æ„ç¤ºä¾‹**
   ```bash
   cpp_knowledge_base/
   â”œâ”€â”€ standard_library/
   â”‚   â”œâ”€â”€ containers/
   â”‚   â”‚   â”œâ”€â”€ vector.md
   â”‚   â”‚   â””â”€â”€ map.md
   â”‚   â””â”€â”€ algorithms/
   â”‚       â”œâ”€â”€ sort.md
   â”‚       â””â”€â”€ find.md
   â”œâ”€â”€ language_features/
   â”‚   â”œâ”€â”€ templates.md
   â”‚   â””â”€â”€ lambda.md
   â””â”€â”€ index.md  # è‡ªåŠ¨ç”Ÿæˆçš„ç›®å½•ç´¢å¼•
   ```

5. **éƒ¨ç½²å»ºè®®**
   - ä½¿ç”¨Scrapyæ¡†æ¶æ›¿ä»£åŸºç¡€çˆ¬è™«
   - æ·»åŠ å¢é‡çˆ¬å–åŠŸèƒ½
   - ä½¿ç”¨Celeryè¿›è¡Œä»»åŠ¡é˜Ÿåˆ—ç®¡ç†
   - æ·»åŠ è‡ªåŠ¨ç”Ÿæˆç›®å½•ç´¢å¼•çš„åŠŸèƒ½

---

### å¤‡é€‰æ–¹æ¡ˆ

1. **æ— APIæ–¹æ¡ˆ**
   - ç¿»è¯‘ï¼šä½¿ç”¨ `translate` åº“ï¼ˆåŸºäºGoogleç¿»è¯‘ï¼‰
   - å¢å¼ºï¼šä½¿ç”¨æœ¬åœ°LLMï¼ˆå¦‚LLaMA.cppï¼‰
   ```python
   # æ›¿ä»£ç¿»è¯‘æ–¹æ¡ˆ
   from translate import Translator
   translator = Translator(to_lang="zh")
   section["translated_content"] = translator.translate(section["content"])
   ```

2. **é™æ€ç«™ç‚¹æ–¹æ¡ˆ**
   - å°†ç”Ÿæˆçš„MDæ–‡ä»¶ç”¨MkDocsæ„å»ºé™æ€ç½‘ç«™
   - æ·»åŠ æœç´¢åŠŸèƒ½ï¼ˆä½¿ç”¨lunr.jsï¼‰

3. **å•†ä¸šåŒ–æ–¹æ¡ˆ**
   - ä½¿ç”¨ç°æˆçš„æ–‡æ¡£å¹³å°ï¼ˆå¦‚Read the Docsï¼‰
   - åˆ©ç”¨Confluenceç­‰ä¼ä¸šçŸ¥è¯†åº“å·¥å…·

---

æ­¤æ–¹æ¡ˆå¯å®ç°è‡ªåŠ¨åŒ–æŠ“å–ã€ç¿»è¯‘ã€å¢å¼ºå’Œæ–‡æ¡£ç”Ÿæˆï¼Œå»ºè®®å…ˆä»æ ¸å¿ƒæ¨¡å—ï¼ˆå¦‚STLå®¹å™¨ï¼‰å¼€å§‹è¯•ç‚¹ï¼Œå†é€æ­¥æ‰©å±•ã€‚å®Œæ•´å®ç°çº¦éœ€500-800è¡ŒPythonä»£ç ï¼Œå…³é”®ç‚¹åœ¨äºå†…å®¹è§£æçš„å‡†ç¡®æ€§å’Œæœ¯è¯­ä¸€è‡´æ€§å¤„ç†ã€‚

---

### åŸºäº Dify çš„å®Œæ•´è§£å†³æ–¹æ¡ˆï¼šè‡ªåŠ¨åŒ– Cppreference çŸ¥è¯†åº“æ„å»º

åˆ©ç”¨ Dify çš„æ™ºèƒ½ä½“å·¥ä½œæµèƒ½åŠ›ï¼Œæˆ‘ä»¬å¯ä»¥æ„å»ºä¸€ä¸ªç«¯åˆ°ç«¯çš„è‡ªåŠ¨åŒ–ç³»ç»Ÿï¼Œä»¥ä¸‹æ˜¯å®Œæ•´æ–¹æ¡ˆï¼š

```mermaid
graph TD
    A[è§¦å‘å·¥ä½œæµ] --> B[çˆ¬è™«æ™ºèƒ½ä½“]
    B --> C[è§£ææ™ºèƒ½ä½“]
    C --> D[ç¿»è¯‘æ™ºèƒ½ä½“]
    D --> E[å¢å¼ºæ™ºèƒ½ä½“]
    E --> F[æ–‡æ¡£ç”Ÿæˆæ™ºèƒ½ä½“]
    F --> G[çŸ¥è¯†åº“å­˜å‚¨]
    G --> H[è‡ªåŠ¨ç´¢å¼•]
```

#### 1. åœ¨ Dify ä¸­åˆ›å»ºæ™ºèƒ½ä½“å·¥ä½œæµ

##### å·¥ä½œæµé…ç½®ï¼ˆYAML æ ¼å¼ï¼‰
```yaml
name: cppreference-knowledge-builder
description: è‡ªåŠ¨åŒ–æŠ“å–ã€ç¿»è¯‘å’Œå¢å¼º Cppreference å†…å®¹
steps:
  - name: crawler_agent
    type: tool
    parameters:
      start_url: "https://en.cppreference.com/w/"
      max_depth: 3
      output_dir: "/data/raw_html"
  
  - name: parser_agent
    type: tool
    depends_on: crawler_agent
    parameters:
      input_dir: "/data/raw_html"
      output_dir: "/data/parsed_json"
  
  - name: translator_agent
    type: llm
    depends_on: parser_agent
    model: gpt-4-turbo
    prompt: |
      ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šC++æ–‡æ¡£ç¿»è¯‘ä¸“å®¶ï¼Œè¯·å°†ä»¥ä¸‹C++å‚è€ƒå†…å®¹å‡†ç¡®ç¿»è¯‘æˆä¸­æ–‡ï¼š
      - ä¿ç•™æ‰€æœ‰æŠ€æœ¯æœ¯è¯­çš„è‹±æ–‡åŸåï¼ˆæ‹¬å·ä¸­æ ‡æ³¨ä¸­æ–‡ï¼‰
      - ä¿æŒä»£ç å—å’Œæ•°å­¦å…¬å¼åŸæ ·
      - ç¡®ä¿æŠ€æœ¯å‡†ç¡®æ€§
      å†…å®¹ï¼š
      {{ parser_agent.output }}
  
  - name: enhancer_agent
    type: llm
    depends_on: translator_agent
    model: gpt-4-turbo
    prompt: |
      ä½ æ˜¯ä¸€ä¸ªC++é«˜çº§å·¥ç¨‹å¸ˆï¼Œè¯·å¯¹ä»¥ä¸‹ç¿»è¯‘å†…å®¹è¿›è¡Œå¢å¼ºï¼š
      1. æ·»åŠ é€šä¿—æ˜“æ‡‚çš„è§£é‡Š
      2. è¡¥å……1-2ä¸ªå®ç”¨ä»£ç ç¤ºä¾‹
      3. æ·»åŠ æ³¨æ„äº‹é¡¹å’Œä½¿ç”¨åœºæ™¯
      4. æ€»ç»“æ ¸å¿ƒè¦ç‚¹ï¼ˆä¸è¶…è¿‡3ç‚¹ï¼‰
      
      åŸå§‹æ ‡é¢˜: {{ parser_agent.metadata.title }}
      ç¿»è¯‘å†…å®¹:
      {{ translator_agent.output }}
  
  - name: md_generator
    type: tool
    depends_on: enhancer_agent
    parameters:
      template: |
        # {{ title }}
        
        ## ğŸ“š åŸå§‹å‚è€ƒ
        {{ original_summary }}
        
        ## ğŸŒŸ å¢å¼ºè§£æ
        {{ enhanced_content }}
        
        ### ä»£ç ç¤ºä¾‹
        {{ code_examples }}
        
        ### ğŸ’¡ æ ¸å¿ƒè¦ç‚¹
        {{ key_points }}
        
        [æŸ¥çœ‹åŸå§‹æ–‡æ¡£]({{ source_url }})
  
  - name: knowledge_store
    type: tool
    depends_on: md_generator
    parameters:
      output_dir: "/knowledge_base/{{ category }}/{{ topic }}"
      format: markdown
      index_strategy: auto-categorize
```

#### 2. åˆ›å»ºè‡ªå®šä¹‰å·¥å…·æ™ºèƒ½ä½“

##### çˆ¬è™«æ™ºèƒ½ä½“ (Python)
```python
import requests
from bs4 import BeautifulSoup
import os
import json
from dify_client import DifyTool

class CppRefCrawler(DifyTool):
    def execute(self, params: dict) -> dict:
        start_url = params['start_url']
        max_depth = int(params.get('max_depth', 2))
        output_dir = params['output_dir']
        
        os.makedirs(output_dir, exist_ok=True)
        crawled = set()
        
        def crawl(url, depth=0):
            if depth > max_depth or url in crawled:
                return
                
            try:
                response = requests.get(url, headers={'User-Agent': 'DifyBot/1.0'})
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # ä¿å­˜åŸå§‹HTML
                filename = os.path.join(output_dir, f"{url.split('/')[-1]}.html")
                with open(filename, 'w', encoding='utf-8') as f:
                    f.write(response.text)
                
                # æå–å…ƒæ•°æ®
                title = soup.find('h1').text.strip()
                category = url.split('/')[-2] if len(url.split('/')) > 4 else 'general'
                
                # æŸ¥æ‰¾ç›¸å…³é“¾æ¥
                for link in soup.select('a[href^="/w/"]'):
                    next_url = f"https://en.cppreference.com{link['href']}"
                    if '#cite_' not in next_url:
                        crawl(next_url, depth+1)
                
                crawled.add(url)
                return {'url': url, 'title': title, 'category': category}
                
            except Exception as e:
                return {'error': str(e)}
        
        result = crawl(start_url)
        return {'files': list(crawled), 'metadata': result}
```

##### è§£ææ™ºèƒ½ä½“ (Python)
```python
class CppRefParser(DifyTool):
    def execute(self, params: dict) -> dict:
        input_dir = params['input_dir']
        output_dir = params['output_dir']
        os.makedirs(output_dir, exist_ok=True)
        
        results = []
        
        for file in os.listdir(input_dir):
            if file.endswith('.html'):
                with open(os.path.join(input_dir, file), 'r', encoding='utf-8') as f:
                    soup = BeautifulSoup(f.read(), 'html.parser')
                    
                    # æå–æ ¸å¿ƒå†…å®¹
                    content = soup.select_one('#content')
                    
                    # è§£æç« èŠ‚
                    sections = []
                    for section in content.select('.t-section, .t-dsc'):
                        title = section.find(['h2', 'h3', 'h4'])
                        if not title: continue
                            
                        # æ¸…ç†å†…å®¹
                        for elem in section.select('.t-navbar, .t-dsc-see'):
                            elem.decompose()
                            
                        text = section.get_text(separator='\n').strip()
                        code_blocks = [pre.text.strip() for pre in section.select('pre')]
                        
                        sections.append({
                            'heading': title.text.strip(),
                            'level': int(title.name[1]),
                            'content': text,
                            'code_blocks': code_blocks
                        })
                    
                    # ä¿å­˜ç»“æ„åŒ–æ•°æ®
                    filename = file.replace('.html', '.json')
                    with open(os.path.join(output_dir, filename), 'w', encoding='utf-8') as f:
                        json.dump({
                            'title': soup.find('h1').text.strip(),
                            'url': f"https://en.cppreference.com/w/{file[:-5]}",
                            'sections': sections
                        }, f, ensure_ascii=False)
                    
                    results.append(filename)
        
        return {'parsed_files': results}
```

#### 3. éƒ¨ç½²åˆ° Dify çš„å·¥ä½œæµ

1. **åˆ›å»ºå·¥å…·æ™ºèƒ½ä½“**ï¼š
   - åœ¨ Dify çš„ "å·¥å…·" é¡µé¢åˆ›å»ºæ–°å·¥å…·
   - ä¸Šä¼ çˆ¬è™«å’Œè§£ææ™ºèƒ½ä½“ä»£ç 
   - é…ç½®è¾“å…¥/è¾“å‡ºå‚æ•°

2. **é…ç½®LLMæ™ºèƒ½ä½“**ï¼š
   - åˆ›å»ºç¿»è¯‘å’Œå¢å¼ºä¸¤ä¸ªLLMæ™ºèƒ½ä½“
   - ä½¿ç”¨é¢„è®¾çš„æç¤ºè¯æ¨¡æ¿
   - è®¾ç½®æ¸©åº¦å‚æ•°ä¸º0.3ä»¥ä¿è¯æŠ€æœ¯å‡†ç¡®æ€§

3. **æ„å»ºå·¥ä½œæµ**ï¼š
   - æŒ‰é¡ºåºè¿æ¥å„æ™ºèƒ½ä½“
   - é…ç½®é”™è¯¯å¤„ç†ç­–ç•¥ï¼ˆé‡è¯•/è·³è¿‡ï¼‰
   - è®¾ç½®é€Ÿç‡é™åˆ¶ï¼ˆé¿å…APIè¿‡è½½ï¼‰

4. **çŸ¥è¯†åº“é›†æˆ**ï¼š
   ```python
   class KnowledgeStore(DifyTool):
       def execute(self, params: dict) -> dict:
           content = params['content']
           category = params.get('category', 'general')
           topic = params['title'].replace(' ', '_')
           
           # åˆ›å»ºåˆ†ç±»ç›®å½•
           category_dir = os.path.join(params['output_dir'], category)
           os.makedirs(category_dir, exist_ok=True)
           
           # ä¿å­˜Markdown
           filename = f"{topic}.md"
           with open(os.path.join(category_dir, filename), 'w', encoding='utf-8') as f:
               f.write(content)
           
           # æ›´æ–°ç´¢å¼•
           self.update_index(category, topic, params['url'])
           
           return {'stored_path': os.path.join(category, filename)}
       
       def update_index(self, category, topic, url):
           index_file = os.path.join(params['output_dir'], 'INDEX.md')
           if not os.path.exists(index_file):
               with open(index_file, 'w') as f:
                   f.write("# C++ çŸ¥è¯†åº“ç´¢å¼•\n\n")
           
           with open(index_file, 'a', encoding='utf-8') as f:
               f.write(f"- [{category}/{topic}]({url})\n")
   ```

#### 4. é«˜çº§ä¼˜åŒ–ç­–ç•¥

1. **å¢é‡æ›´æ–°æœºåˆ¶**ï¼š
   ```python
   # åœ¨çˆ¬è™«æ™ºèƒ½ä½“ä¸­æ·»åŠ 
   def get_last_crawl_time(url):
       # ä»æ•°æ®åº“æˆ–æ–‡ä»¶ä¸­è·å–ä¸Šæ¬¡çˆ¬å–æ—¶é—´
       return last_modified
   
   if datetime.now() - get_last_crawl_time(url) < timedelta(days=7):
       return {'status': 'skipped', 'reason': 'recently_updated'}
   ```

2. **æœ¯è¯­ä¸€è‡´æ€§ç®¡ç†**ï¼š
   - åˆ›å»ºå…±äº«æœ¯è¯­è¡¨å·¥å…·
   ```python
   class TermManager(DifyTool):
       term_base = {}
       
       def execute(self, params: dict):
           term = params['term']
           if term not in self.term_base:
               # ä½¿ç”¨GPTç”Ÿæˆæ ‡å‡†ç¿»è¯‘
               translation = llm_query(f"æ ‡å‡†åŒ–ç¿»è¯‘æŠ€æœ¯æœ¯è¯­: {term}")
               self.term_base[term] = translation
           return self.term_base[term]
   ```
   - åœ¨ç¿»è¯‘æ™ºèƒ½ä½“ä¸­è°ƒç”¨æœ¯è¯­ç®¡ç†å·¥å…·

3. **è´¨é‡éªŒè¯æµç¨‹**ï¼š
   ```yaml
   - name: quality_check
     type: llm
     depends_on: md_generator
     prompt: |
       ä½œä¸ºC++ä¸“å®¶ï¼Œè¯·éªŒè¯ä»¥ä¸‹å†…å®¹çš„æŠ€æœ¯å‡†ç¡®æ€§ï¼š
       1. ä»£ç ç¤ºä¾‹æ˜¯å¦èƒ½ç¼–è¯‘è¿è¡Œ
       2. æ¦‚å¿µè§£é‡Šæ˜¯å¦å‡†ç¡®
       3. æ˜¯å¦å­˜åœ¨è¿‡æ—¶ä¿¡æ¯
       
       æ–‡æ¡£å†…å®¹:
       {{ md_generator.output }}
       
       è¯·ç”¨JSONæ ¼å¼è¿”å›éªŒè¯ç»“æœå’Œä¿®æ”¹å»ºè®®
   ```

#### 5. éƒ¨ç½²æ¶æ„

```
Docker å®¹å™¨æ¶æ„ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Dify Server   â”‚â—„â”€â”€â”€â”€â”€â–ºâ”‚  Tool Agents    â”‚
â”‚    (ä¸»æ§)       â”‚       â”‚  (Pythonå®¹å™¨)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                           â”‚
       â–¼                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LLM Gateway    â”‚       â”‚ Knowledge Store â”‚
â”‚ (GPT/Claude API)â”‚       â”‚  (MinIO/S3)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 6. ç›‘æ§ä¸ç»´æŠ¤

1. **ç›‘æ§é¢æ¿**ï¼š
   - è·Ÿè¸ªæ¯ä¸ªæ™ºèƒ½ä½“çš„æ‰§è¡Œæ—¶é—´å’ŒæˆåŠŸç‡
   - è®°å½•æœ¯è¯­åº“å¢é•¿æƒ…å†µ
   - ç›‘æ§APIä½¿ç”¨æˆæœ¬

2. **è‡ªåŠ¨ç»´æŠ¤ä»»åŠ¡**ï¼š
   - æ¯å‘¨éªŒè¯çŸ¥è¯†åº“é“¾æ¥æœ‰æ•ˆæ€§
   - æ¯æœˆæ›´æ–°C++æ ‡å‡†å˜åŒ–éƒ¨åˆ†
   - å­£åº¦æ€§è´¨é‡å®¡è®¡ï¼ˆæŠ½æ ·æ£€æŸ¥ï¼‰

3. **äººå·¥å¹²é¢„ç‚¹**ï¼š
   - æ–°C++æ ‡å‡†å‘å¸ƒæ—¶æ‰‹åŠ¨è§¦å‘å…¨é‡æ›´æ–°
   - æœ¯è¯­äº‰è®®æ—¶äººå·¥ç¡®è®¤
   - å¤æ‚æ¨¡æ¿ç‰¹åŒ–å†…å®¹çš„æ‰‹åŠ¨å®¡æ ¸

æ­¤æ–¹æ¡ˆå……åˆ†åˆ©ç”¨äº† Dify çš„æ™ºèƒ½ä½“ç¼–æ’èƒ½åŠ›ï¼Œå°†å¤æ‚æµç¨‹åˆ†è§£ä¸ºå¯ç®¡ç†çš„æ­¥éª¤ï¼ŒåŒæ—¶ä¿æŒäº†ç³»ç»Ÿçš„çµæ´»æ€§å’Œå¯æ‰©å±•æ€§ã€‚éƒ¨ç½²åå¯é€šè¿‡ Dify çš„ API æˆ– Webhook è§¦å‘å·¥ä½œæµï¼Œæœ€ç»ˆç”Ÿæˆçš„çŸ¥è¯†åº“å¯ç›´æ¥ç”¨äº MkDocsã€Docusaurus ç­‰æ–‡æ¡£ç³»ç»Ÿã€‚